
\rule{7em}{.4pt}

Sia\marginnote{13 dic 2021} $ (V, \cdot ) $ uno spazio vettoriale Euclideo $\implies$ $ \cdot  $ induce un isomorfismo da $ V $ a $ V^{*} $; infatti , ogni $ v \in V $ induce $ v \cdot \in V^{*} $ definito come \[
    v\!\cdot (w):= v \cdot w
\]
Quindi è definita \begin{align*}
\phi:V & \to V^{*} \\
v & \mapsto v \cdot 
\end{align*} $ \phi $ è lineare e $ \ker\phi=\{\underline{0}\} $ infatti se $ v \in \ker\phi $ 

$\implies$ $ v \cdot = \underline{0}_{V^{*}}  $, quindi $ v \cdot w= 0 $ $ \forall\, w \in W $ 

$\implies$ $ v=\underline{0} $ poiché $ \cdot  $ è definito positivo.

$ \phi $ è una funzione lineare iniettiva, poiché $ \dim V = \dim  V^{*} $, $ \phi $ è un isomorfismo.

\definizione{}{
    Un isomorfismo tra $ V $ e $ V^{*} $ si dice una \textit{dualità}.
}

\rule{7em}{.4pt}

Siano $ V $ e $ W $ spazi vettoriali sullo stesso campo $ \K $, e sia $ F:V\to W $ lineare 

$\implies$ $ F $ induce $ F^{*}:W^{*}\to V^{*} $ definita dalla relazione \[
    \beta \in W^{*}\qquad F^{*} (\beta) \in V^{*},\quad F^{*}(\beta)(v):= \beta\bigl(F(v)\bigr)\quad \forall\, v \in V 
\] In questo modo $ F^{*} $ è una funzione lineare.

Supponiamo ora che $ V $ e $ W $ abbiano dimensione finita, sia $ F:V\to W $ lineare. Siano $ \mathscr{B}=\{v_1, \cdots, v_{n} \} $ base di $ V $ e $ \mathscr{C}=\{w_{1}, \cdots, w_{m}\}$ base di $ W $. 

Abbiamo $ \mathscr{B}^{*} $ base di $ V^{*} $, e $ \mathscr{C}^{*} $ base di $ W^{*} $.

Abbiamo $ F^{*}:W^{*}\to V^{*} $ lineare, e $ M^{ \mathscr{B}, \mathscr{C}}(F)$ e $ M^{ \mathscr{C}^{*}, \mathscr{B}^{*}} (F^{*}) $.

\proposizione{matricedelvattelapersca}{
    $ M^{ \mathscr{C}^{*}, \mathscr{B}^{*}} (F^{*}) = \null^{t}\!M^{ \mathscr{B}, \mathscr{C}}(F)$
}
\dimostrazioneprop{matricedelvattelapersca}{
    Sia $ \mathscr{B}^{*}=\{v_1^{*}, \cdots, v_{n}^{*}\}  $ e $ \mathscr{C}^{*}=\{w_1^{*}, \cdots, w_{m}^{*}\}  $,\[\null^{t}\!M^{ \mathscr{B}, \mathscr{C}}(F) = (a_{ij})\] e sappiamo \[
        F(v_{j} )=\sum_{i=1}^{m} a_{ij} w_{i}  
    \]

    Calcoliamo ora \begin{multline*}
        \bigl(F^{*}(w^{*}_i)\bigr)(v_{j} )=w_{j}^{*} \bigl(F(v_{j} )\bigr) =\\
        =w_{j}^{*}\bigl(\sum_{k=1}^{m} a_{kj} w_{k}   \bigr) =\\
        = \sum_{k=1}^{m} a_{kj}\:\parentesi{\delta_{ij}}{w_{i}^{*}(w_{k} ) }  = a_{ij} 
    \end{multline*} Ne deduco che \[
        F^{*}(w_{i}^{*} )(v_{j} )= a_{ij} 
    \] 
    
    $\implies$ $ F^{*}(w_{i} )=\sum_{j=1}^{n} a_{ij}\, v_{j}^{*}   $ 
    
    $\implies$ $ M^{ \mathscr{C}^{*}, \mathscr{B}^{*}} (F^{*}) = \null^{t}\!M^{ \mathscr{B}, \mathscr{C}}(F)$.\qed
}

\rule{7em}{.4pt}

Sia $ V $ spazio vettoriale su un campo $ \K $ e $ H \subseteq V $ sottospazio vettoriale. Definiamo \[
    H^{0}=\{\alpha \in V^{*}\,\tc\, \alpha(v)= 0 \: \forall\, v \in H \}
\] e si dice \textit{l'annullatore} di $ H $.

$ H^{0} \in V^{*} $ e $ H^{0} $ è un sottospazio vettoriale.

\esempio{
    Sia $ V= \R^{3} $, $ H= \mathscr{L}((1,2,1)) $, trovare $ H^{0} $. 
    
    Fisso $ \mathscr{B}=\{e_1, e_2, e_3\} $ base canonica di $ \R^{3}$, e $ \mathscr{B}^{*}=\{e_1^{*}, e_2^{*}, e_3^{*}\} $ base duale a $ \mathscr{B} $. \[
        H^{0}=\{\alpha \in V^{*}\,\tc\, \alpha(v)= 0 \: \forall\, v \in H \}=\{\alpha \in V^{*}\,\tc\, \alpha((1,2,1))=0\}
    \] Un $ \alpha $ generico di $ V^{*} $ si scrive come \[
        \alpha= a_1 e_1^{*} + a_2 e_2^{*} + a_3e_3^{*}
    \]\begin{multline*}
        \alpha((1,2,1))=\alpha(e_1+2e_2+e_3)=\\
        = (a_1 e_1^{*} + a_2 e_2^{*} + a_3e_3^{*})(e_1+2e_2+e_3)=\\
        = a_1+2s_2+a_3
    \end{multline*}

    $ \alpha $ si annulla in $ H $ $ \iff $ $ a_1+2a_2+a_3=0$ \[
        H^{0}=\{a_1 e_1^{*} + a_2 e_2^{*} + a_3e_3^{*}\, |\, a_1+2a_2+a_3=0\}
    \] quindi $ \dim H^{0}= 2 $.
}

\proposizione{sommadimfinitia}{
    Sia $ V $ uno spazio vettoriale di dimensione finita su un campo $ \K $. Sia $ H \subseteq V $ un sottospazio vettoriale 
    
    $\implies$ $ \dim H + \dim H^{0} = \dim V $
}
\dimostrazioneprop{sommadimfinitia}{
    Sia $ \{v_1, \cdots, v_{h} \} $ una base di $ H $, e la completiamo ad una base di $ V $, nella forma \[
       \mathscr{B}= \{v_1, \cdots, v_{h}, w_1, \cdots, w_{n-h} \}
    \] considero \[
        \mathscr{B}^{*}=\{v_1^{*}, \cdots, v_{h}^{*}, w_1^{*}, \cdots, w_{n-h}^{*} \}
    \] 

    Sappiamo che $ v_{j}^{*}(v_{j} )=1  $ $ \forall\, j = 1, \cdots, h $ 
    
    $\implies$ $ v_{j}^{*}\notin H^{0}  $, invece $ w_{j}^{*}(v_j)=0  $ $ \forall\, j=1, \cdots, h $, $ \forall\, i = 1, \cdots, n-h $ 
    
    $\implies$ $ w_{i}^{*} \in H^{0} $ $ \forall\, i =1, \cdots, n-h  $ 
    
    $\implies$ $ H^{0}= \mathscr{L}(w_1^{*}, \cdots, w_{n-h}^{*} )$ e dato che $ \{w_1^{*}, \cdots, w_{n-h}^{*}\}  $ è libero, risulta che $ \dim H^{0}= n-h = \dim V - \dim H$.\qed
}

\rule{7em}{.4pt}

Siano $ V, W $ spazi vettoriali di dimensione finita sullo stesso campo $ \K $, $ \dim  V=n $, $ \dim W=m $. Sia $ F:V\to W $ lineare, e $ F^{*}:W^{*}\to V^{*} $ la funzione indotta. \[
    \bigl(F^{*}(\beta)\bigr)(v)=\beta\bigl(F(v)\bigr)
\] Calcolo $ \ker F^{*} $ \[
    \ker F^{*}=\{\beta \in W^{*}\,|\, F^{*}(\beta)= \underline{0}_{V^{*}}\}
\] Quindi 

$ \beta \in \ker(F^{*}) $ 

$ \iff $ $ \bigl(F^{*}(\beta)\bigr)(v)=0 $ $ \forall\, v \in V $

$ \iff $ $ \beta \bigl(F(v)\bigr)=0 $ $ \forall\, v \in V $

Quindi $ \ker (F^{*})= F(v)^{0} $, cioè $ \ker F^{*} $ è l'annullatore di $ F(V) $ 
\begin{multline*}
    \implies \dim\ker F^{*} = \dim F(V)^{0} =\\
    = \dim W - \dim  F(V) = \dim W - \dim \rank (F)
\end{multline*} 

%$\implies$ $ \ker (F^{*}) $ e $ \ker(F) $ hanno la stessa dimensione 
% DOMANDA manca un passaggio

$\implies$ $ \rank(F)=\rank(F^{*}) $ 

$\implies$ Se fisso $ \mathscr{B} $ base di $ V $ e $ \mathscr{C} $ base di $ W $ risulta che $ M^{ \mathscr{B}, \mathscr{C}}(F) $ e $ M^{ \mathscr{C}^{*}, \mathscr{B}^{*}}(F^{*})$ hanno lo stesso rango. Ma $ M^{ \mathscr{B}, \mathscr{C}}(F) $ e $ M^{ \mathscr{C}^{*}, \mathscr{B}^{*}}(F^{*})$ sono una la matrice trasposta dell'altra $\implies$
\teorema{belteoremauuuu}{
    Sia $ A \in \K^{m,n} $ 
    
    $\implies$ $ A $ e la sua trasposta hanno lo stesso rango \begin{equation}
        \rank (A)=\rank (\null^{t}\!A)
    \end{equation}
}
\osservazione{
    $ V $ a dimensione finita su un campo $ \K $, fissiamo $ \mathscr{B}, \mathscr{C} $ due basi di $ V $, $ P=M^{ \mathscr{B}, \mathscr{C}} $ matrice del cambiamento di base, $ \mathscr{B}^{*}, \mathscr{C}^{*} $ basi di $ V^{*} $\begin{equation}
        M^{ \mathscr{B}^{*}, \mathscr{C}^{*}}=\null^{t}\!P^{-1}
    \end{equation}
}
\begin{proof}
    $P=M^{ \mathscr{B}, \mathscr{C}}(\Id_{V} )$, $ P^{-1}=M^{ \mathscr{C}; \mathscr{B}}(\id_{V} ) $, 
    
    $ M^{ \mathscr{B}^{*}, \mathscr{C}^{*}}(\id_{V}^{*} ) = \null^{t}\!P^{-1} $, ma $ \id_{V}^{*}=\id_{V^{*}}   $ 
    
    $\implies$ $ \null^{t}\!P^{-1}=  M^{ \mathscr{B}^{*}, \mathscr{C}^{*}}(\id_{V^{*}} ) =  M^{ \mathscr{B}^{*}, \mathscr{C}^{*}} $
\end{proof}

\section{Coniche}

\definizione{}{
    Una \textit{conica} è il luogo degli zeri in $ \R^{2} $ di un polinomio di secondo grado in due variabili.
}

$ \R[x,y] $ è l'insieme dei polinomi nelle variabili $ x,y $ a coeiffienti in $ \R $. Tutti i polinomi di grado due in $ (x,y) $ sono descritti come: 
\[
    p(x,y)=a_{11} x^{2} + 2 a_{12} xy + a_{22}y^{2} + 2 a_{13}x + 2 a_{23} y + a_{33}
\]
con $ (a_{11}, a_{12}, a_{22})\neq (0,0,0) $

Scopo della teoria: si applica una rototraslazione del piano (una composizione di una traslazione con una rotazione) e si riporta la conica ad una delle seguenti forme canoniche
\begin{enumerate}
    \item $\displaystyle\frac{x^{2}}{a^{2}}+\frac{y^{2}}{b^{2}}=1$ con $ a,b\neq 0 $: ellissi reali (la conica è un'ellissi);
    \item $\displaystyle\frac{x^{2}}{a^{2}}+\frac{y^{2}}{b^{2}}=-1$ con $ a,b\neq 0 $: ellissi immaginarie (la conica è $ \emptyset $);
    \item $\displaystyle\frac{x^{2}}{a^{2}}+\frac{y^{2}}{b^{2}}=0$ con $ a,b\neq 0 $: ellissi degeneri (la conica è $ \underline{0}=(0,0) $);
    \item $\displaystyle\frac{x^{2}}{a^{2}}-\frac{y^{2}}{b^{2}}=1$ con $ a,b\neq 0 $: iperboli;
    \item $\displaystyle\frac{x^{2}}{a^{2}}-\frac{y^{2}}{b^{2}}=0$ con $ a,b\neq 0 $: iperboli degeneri (la conica è unione di due rette);
    \item $ \displaystyle y = ax^{2} $ con $ a\neq 0 $: parabole;
    \item $ x^{2}=h^{2} $ con $ h\neq 0 $: coppie di rette distinte (la conica è unione di due rette);
    \item $ x^{2}=0 $: conica doppiamente degenere (la conica è la retta $ x=0 $);
    \item $ x^{2}=-k^{2} $ con $ K\neq 0 $: coppia di rette immaginarie (la conica è $ \emptyset $).
\end{enumerate}
\definizione{}{
    Le forme 1. - 9. si dicono le \textit{forme canoniche}.
}

\teorema{allafaccia}{
    Ogni conica, tramite una rototraslazione, può essere ridotta ad una forma canonica.
}

Per ridurre una conica in forma canonica si parte dal polinomio \[
    p(x,y)=a_{11} x^{2} + \textcolor{red}{2 a_{12}} xy + a_{22}y^{2} + 2 a_{13}x + 2 a_{23} y + a_{33}
\] e si distinguono due casi: $ a_{12}=0 $ e $ a_{12}\neq 0 $

Nel caso $ a_{12}=0 $ basta una traslazione, nel caso $ a_{12}\neq 0 $ serve una rotazione per riportarci al caso precedente.

\esempio{
    \begin{gather*}
        x^{2}+2y^{2}+4x+4y-2=0\qquad (x+2)^{2} = x^{2}+4x+4\\
        \textcolor{red}{x^{2}}+2y^{2}+\textcolor{red}{4x}+4y+\textcolor{red}{4}-4-2=0\\
        (x+2)^{2}+2y^{2}+4y-6=0\\
        (x+2)^{2}=2(y^{2}+2y)-6=0\qquad (y+1)^{2}= y^{2}+2y+1\\
        (x+2^{2})+2\bigl((y+1)^{2}-1\bigr)-6=0\\
        (x+2)^{2}+2(y+1)^{2}-8=0
    \end{gather*}
    Ora si pone $ X=x+2 $ e $ Y=y+1 $ (ovvero applico una traslazione): \[
        X^{2}+2Y^{2}=8
    \] \[
        \frac{X^{2}}{8}+ \frac{Y^{2}}{4}=1
    \] forma canonica di un ellisse.
}

Questo procedimento (del completamento dei quadrati) si può fare ogni qualvolta il coefficiente di $ xy $ sia nullo. (\underline{caso $ a_{12}=0 $})

Nel \underline{caso $ a_{12}=0$} si applica una rotazione in modo da riportarsi al caso precedente.
\[
    a_{11} x^{2} + 2 a_{12} xy + a_{22}y^{2} + 2 a_{13}x + 2 a_{23} y + a_{33}=0
\]
Posso vederlo come \[
    \begin{pmatrix}
        x & y
    \end{pmatrix}\parentesi{A\footnotemark}{\begin{pmatrix}
        a_{11} & a_{12}\\
        a_{12} & a_{22}
    \end{pmatrix}}\begin{pmatrix}
        x\\y
    \end{pmatrix} + 2a_{13}+2a_{23}y+a_{33}.
\]\footnotetext{matrice simmetrica detta incompleta della conica}

Si applica una rotazione in modo da scrivere \[
    \begin{pmatrix}
        x\\ y
    \end{pmatrix} = P \begin{pmatrix}
        \tilde{x}\\ \tilde{y}
    \end{pmatrix}
\] con $ P \in SO(2) $, ovvero $ \null^{t}\!P=P^{-1} $ e $ \det P =1 $. La conica diventa
\[
    \begin{pmatrix}
        \tilde{x} & \tilde{y}
    \end{pmatrix}\null^{t}\!P A P\begin{pmatrix}
        \tilde{x}\\\tilde{y}
    \end{pmatrix} + 2a_{13}'+2a_{23}'y+a_{33}.
\] 
Per ``eliminare'' la parte in $ xy $ devo trovare $ P \in SO(2) $ tale che $ \null^{t}\!PAP $ sia diagonale.

Poiché $ A $ è simmetrica posso applicare il teorema spettrale, e so che esiste $ P \in SO(2) $ tale che $ \null^{t}\!P A P =\Bigl(\begin{smallmatrix}
    \lambda_1 & 0\\
    0 & \lambda_2
\end{smallmatrix}\Bigr)\ $ dove $ \lambda_1, \lambda_2  $ sono gli autovalori di $ A$. 

Le colonne di $ P $ foramno una base ortonormale di autovettori di $ A $ e soddisfano $ \det P = 1 $.

% \dimostrazione{allafaccia}{
    
% }