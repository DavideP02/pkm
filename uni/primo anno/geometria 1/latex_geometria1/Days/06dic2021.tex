\subsection{Vettori ortogonali}
\definizione{Data\marginnote{6 dic 2021} $ \xi \in B(V, \K)$, $ v, w $ sono ortogonali a $ \xi $ se $ \xi(v,w)=0 $}

\osservazione{
    Il vettore nullo $ \underline{0} $ è ortogonale ad ogni $ v \in V $, infatti \[\xi(v, \underline{0})=\xi(v, 0 \cdot \underline{0})=0\,\xi(v, \underline{0})=0\]
}

Sia $ A \subseteq V $ un sottoinsieme, \[
    A^{\bot}=\{v \in V\,\tc\, \xi(a, v)=0\, \forall\, a \in A\}
\] $ A^{\bot} $ si dice \textit{spazio ortogonale} ad $ A $

\proposizione{elefantedivetroaustriaco}{
    $ A^{\bot} $ è sempre un sottospazio vettoriale
}
\dimostrazioneprop{elefantedivetroaustriaco}{
    Siano $ v, w \in A^{\bot} $, $ \lambda, \mu \in \K $ e verifichiamo che $ \lambda v + \mu w \in A^{\bot} $.

    Se $ a \in A $, risulta \[
        \xi(a, \lambda v+\mu w)\underset{\footnotemark}{=}\lambda \xi(a, v) + \mu\xi(a, w)
        = \lambda 0 + \mu 0 =0
    \]\footnotetext{$\xi$ è bilineare} 
    
    $\implies$ $ \lambda v + \mu w \in A^{\bot} $\qed
}

In particolare, se $ H \subseteq V $ è un sottospazio 

$\implies$ $ H^{\bot}  $ è un sottospazio.

\proposizione{eleonorrugnyeeiiddoijooijoijoo}{
    Siano $ v_1, \cdots,v_{l}  \in V$, $ \xi \in B(V, \K) $. Sono fatti equivalenti \begin{enumerate}
        \item $ v \in V $ ortogonale a tutti i $v_{i}$, $ \forall\, i=1, \cdots, l $
        \item $ v $ è ortogonale a $ \mathscr{L}(v_1, \cdots, v_{l} ) $
    \end{enumerate} 
}
\dimostrazioneprop{eleonorrugnyeeiiddoijooijoijoo}{
    \begin{itemize}
        \item [``2. $\implies$ 1.''] È ovvio.
        \item [``1. $\implies$ 2.''] Sia $ w \in \mathscr{L}(v_1, \cdots, v_{l} ) $ 
        
        $\implies$ $ w= \lambda_1 v_1+\cdots+\lambda_{l}v_{l}$, quindi \[
            \xi(w,v)=\xi(\lambda_1 v_1+\cdots+\lambda_{l}v_{l}, v)= \lambda_1 \xi(v_1, w)+\cdots+\lambda_{l} \xi(v_{l}, w )=0\qedhere
        \]
    \end{itemize}
}

Sia $ \xi \in B(V, \K) $, siano $ W_1, W_2 $ sottospazi vettoriali di $ V $, $ W_1 $ e $ W_2 $ sono ortogonali se \[
    \xi(w_1, w_2)=0\quad \forall\, w_1 \in W_1, w_2 \in W_2
\]

\osservazione{
    Se $ V $ è uno spazio vettoriale su $ R $, e $ \xi = \cdot  $ è un prodotto scalare, allora $ \forall\, W \subseteq V $ sottospazio vettoriale, vale: \[
        V= W\oplus W^{\bot}
    \]

    Questo non è vero in generale per le forme bilineari: in molti casi\[W \cap W^{\bot} \neq \{\underline{0}\}\]
}

\esempio{}{
    In $ \R^{3} $ si considera la forma bilineare simmetrica avente forma quadratica \[
        Q(x)=x_1^{2}-2x_3^{2}-4x_1x_2-2x_1x_3-4x_2x_3
    \]
    Sia $ W \subseteq \R^{3} $ il sottospazio vettoriale generato da \[ u_1=(4,1,0)\qquad u_2=(3,0,1) .\] Calcoliamo $ W^{\bot} $.

    Sappiamo che $ \xi(X,Y)=\null^{t}\!X A Y $, dove \[
        A=\begin{pmatrix}
            -1 & -2 & -1\\
            -2 & 0 & -1\\
            -1 & -1 & -2
        \end{pmatrix}
    \]
    Sappiamo che $ x \in W^{\bot} $ $ \iff $ $ \xi(x, u_1)=\xi(x, u_2)=0 $ \begin{multline*}
        \xi (x, u_1)=\\= \begin{pmatrix}
            x_1 & x_2 & x_3
        \end{pmatrix}\begin{pmatrix}
            -1 & -2 & -1\\
            -2 & 0 & -1\\
            -1 & -1 & -2
        \end{pmatrix}\begin{pmatrix}
            4\\1\\0
        \end{pmatrix}=\\
        =\begin{pmatrix}
            x_1 & x_2 & x_3
        \end{pmatrix}\begin{pmatrix}
            2\\-8\\-5
        \end{pmatrix}=\\=
        2x_1-8x_2-5x_3
    \end{multline*}
    \begin{multline*}
        \xi (x, u_2)=\\= \begin{pmatrix}
            x_1 & x_2 & x_3
        \end{pmatrix}\begin{pmatrix}
            -1 & -2 & -1\\
            -2 & 0 & -1\\
            -1 & -1 & -2
        \end{pmatrix}\begin{pmatrix}
            3\\0\\1
        \end{pmatrix}=\\
        =\begin{pmatrix}
            x_1 & x_2 & x_3
        \end{pmatrix}\begin{pmatrix}
            2\\-7\\-5
        \end{pmatrix}=\\
        =2x_1-7x_2-5x_3
    \end{multline*} 
    
    $\implies$ $ x \in W^{\bot} $ $ \iff $ \[
        \begin{cases}
            2x_1-8x_2-5x_3=0\\
            2x_1-7x_2-5x_3=0
        \end{cases}
    \]

    Si risolve il sistema $\implies$ $ \begin{cases}
        x_1=5x_3/2\\
        x_2=0\\
        x_3=x_3
    \end{cases} $ 
    
    $\implies$ $ W^{\bot}= \mathscr{L}(5/2, 0, 1) $
}

\osservazione{
    Se $ v \in W\cap W^{\bot} $, e $ v\neq \underline{0} $ 
    
    $\implies$ $ \xi (v,v)=\underline{0} $. Se $ \mathscr{B} $ è una base di $ V $, risulta che \[
        \null^{t}\!(v)_{ \mathscr{B}}\, M^{ \mathscr{B}}(\xi)\,(v)_{ \mathscr{B}}=0
    \] con $ (v)_{ \mathscr{B}}\neq \underline{0} $ in $ \K^{n} $ 
    
    $\implies$ $ \exists\, X\in \K^{n} $ con $X\neq \underline{0} $ tale che \[
        \null^{t}\!X\, M^{ \mathscr{B}}(\xi)\,X=0.
    \]
}
\subsection{Nucleo di una forma bilineare simmetrica}

Sia $ \xi \in B_{S}(V, \K)  $, \[\ker \xi =\big\{v \in V\,\tc\, \xi(v, w)= 0\, \forall\,w \in V\big\}\]

\esempio{}{
    Se $ (V, \cdot ) $ è uno spazio vettoriale Euclideo e $ \xi= \cdot  $, $\ker \xi =\{\underline{0}\}$
}

\esercizio{Verificare che $ \ker \xi $ è sempre un sottospazio vettoraile di $ V $}{
    Risolvere per esercizio %TODO risolvere 
}{}

\osservazione{
    $ \ker(\xi)^{\bot}= V $, infatti \[\ker(\xi)^{\bot}=\big\{v \in V\,\tc\, \xi(v,w)= 0\, \forall\, w \in \ker(\xi)\big\}=V\]
}

\osservazione{
    In generale se $ W \subseteq V $ è un sottospazio vettoriale, può accadere che \[
        (W^{\bot})^{\bot}\neq W
    \]
}

\esercizio{
    Su $ \R^{3} $ si consideri la forma bilineare simmetrica \[
        \xi(x,y)=2x_1y_1-(x_1y_1+x_2y_2)+x_1y_3+x_3y_1+x_2y_3+x_3y_2-4x_3y_3
    \] e il sottospazio vettoriale \[
        W=\{X \in \R^{3}\,\tc\, x_1+x_2-x_3=0\}
    \]

    Si calcoli $ (W^{\bot})^{\bot} $
}{
    Risolvere per esercizio %TODO risolvere
}{}

\teorema{didisoijadfaoivettcampok}{
    Sia $ V $ uno spazio vettoriale su un campo $ \K $, $ V $ finitamente generato, $ \xi \in B_{S}(V, \K)$. Sia $ \mathscr{B} $ una base di $ V $. 
    
    Allora \begin{equation}
        \ker\xi=\big\{v \in V\,\tc\, (v)_{ \mathscr{B}} \in\nulls\big(M^{ \mathscr{B}}(\xi)\big)\big\}
    \end{equation}
    
}
\dimostrazione{didisoijadfaoivettcampok}{
    Sia $ \mathscr{B}=\{v_1, \cdots, v_{n}\}  $\[
        \ker\xi=\{v \in V\,\tc\, \xi(v,w)=0\, \forall\, w \in V\}=\{v \in V\,\tc\, \xi(v, v_{i} )=0\, \forall\, i =1, \cdots, n\}
    \]
    \begin{align*}
        v \in \ker(\xi)&\iff\\
        &\iff \xi(v, v_{i})=0\, \forall\, i = 1, \cdots, n\iff\\
        &\iff \xi(x_1 v_1+\cdots+x_{n}v_{n}, v_{i}   )= 0\, \forall\, i=1, \cdots,n\iff\\
        &\iff \sum_{j=1}^{n}x_{j}\xi(v_i, v_j)=0\, \forall\, i =1, \cdots, n  \iff M^{ \mathscr{B}}(\xi)(v)_{ \mathscr{B}}=\underline{0}\iff\\&\iff (v)_{ \mathscr{B}} \in \nulls\bigl(M^{ \mathscr{B}}(\xi)\bigr)\qedhere
    \end{align*}
}

\definizione{}{
    Una forma bilineare simmetrica $ \xi $ si dice \begin{itemize}
        \item \textit{degenere} se $ \ker \xi \neq \{\underline{0}\} $;
        \item \textit{non degenere} se $ \ker \xi =\{\underline{0}\} $
    \end{itemize}

    Dal teorema precedente risulta che in dimensione finita: 
    \[ \xi  \text{ non degenere}\quad  \iff  \quad \det\big(M^{ \mathscr{B}}(\xi)\big)\neq 0 \]
    (questa condizione non dipende dalla base che si utilizza).
}

\subsection{Vettori isotropi e cono isotropo}

Sia $ V $ spazio vettoriale su campo $ \K$ (con la caratteristica di $ \K $, $ \neq 2 $), $ \xi \in B_{S}(V, \K)  $. Un vettore $ v \in V $ si dice \textit{isotropo} rispetto a $ \xi $ se\begin{equation}
    Q_{\xi}(v)=0 
\end{equation} (cioè $ \xi(v,v)=0 $).

Si definisce \begin{equation}
    I=\big\{v \in V\,\tc\, Q_{\xi}(v)=0\big\} 
\end{equation} ed è il cono isotropo di $ \xi $. 

\osservazione{
Prende il nome di \textit{cono} poiché $ I $, in generale, non è un sottospazio vettoriale (se $ v, w \in I $, $ Q_{\xi}(v+w)\neq Q_{\xi}(v)+Q_{\xi}(w)$) perché non è in generale chiuso rispetto a ``$+$''.

Però se $ v \in I $ e $ \lambda \in \K $ 

$\implies$ $ Q_{\xi}(\lambda v)= \lambda^{2} Q_{\xi}(v)=0$ \[
    \forall\, v \in I, \lambda \in \K, \lambda v \in I.
\]

Quindi $ I $ non è chiuso rispetto a ``$+$'' ma solo rispetto ai prodotti per scalari. Sottoinsiemi di questo tipo si dicono \textit{coni}.}

\osservazione{
    $ \ker\xi \subseteq I $, infatti se $ v \in \ker \xi $ 
    
    $\implies$ $ \xi(v,w)=0 $ $ \forall\, w \in V $ 
    
    $\implies$ $ \xi(v,v)=0 $ 
    
    $\implies$ $ v \in I $.
}

Se $ V $ ha dimensione finita, fisso $ \mathscr{B}=\{v_1, \cdots, v_{n}\}  $ base di $ V $. \[
    I=\{v \in V\,\tc\, Q_{\xi}(v)=0\}=\{v \in V\,\tc\, \null^{t}\!(v)_{ \mathscr{B}}\, M^{ \mathscr{B}}(\xi)(v)_{ \mathscr{B}}=0\} 
\] Noto che $ \null^{t}\!(v)_{ \mathscr{B}}\, M^{ \mathscr{B}}(\xi)(v)_{ \mathscr{B}}=0 $ è un'equazione di secondo grado nelle componenti di $ (v)_{ \mathscr{B}} $

\esempio{
    Sia su $ \R^{2} $ la forma quadratica $ Q_{\xi}(x)=x_1^{2}-x_2^{2}  $ 
    \begin{multline*}
        I=\{x \in \R^{2}\,\tc\, Q_{\xi}(v)=0 \}= \{x \in \R^{2}\,\tc\, x_1^{2}=x_2^{2}\}=\\
        =\{x \in \R^{2}\,\tc\, x_1=\pm x_2\}=\\=
        \{x \in \R^{2}\,\tc\, x_1=+x_2\}\cup\{x \in \R^{2}\,\tc\, x_1=- x_2\}=\\
        = \mathscr{L}((1,1))\cup \mathscr{L}((1, -1))
    \end{multline*}

    $ I $ è unione di due rette, $ I $ non è sun sottospazio vettoriale di $ \R^{2} $.
}

\teorema{adolfosignorini}{
    Sia $ V $ spazio vettoriale su campo $ \K $, e $ \xi \in B_{S}(V, \K)$ non degenere, sia $ W \subseteq V $ un sottospazio vettoriale. 
    
    $\implies$ $ \dim (W^{\bot})=\dim V-\dim W $
}
\dimostrazione{adolfosignorini}{
    Sia $ \mathscr{B}=\{v_1, \cdots, v_{n}\}  $ una base di $ V $ e supponiamo $ \dim W=h $.
    \begin{itemize}
        \item $ h=0 $ $ \implies $ $ \dim W=0 $ 
        
        $\implies$ $ W=\{\underline{0}\} $ 
        
        $\implies$ $ W^{\bot}=V $ 
        
        $\implies$ $ \dim W^{\bot}= \dim  V -0$
        \item $ h\neq 0 $. Sia $ \{w_1,\cdots, w_{h} \} $ una base di $ W $, sia $ A=M^{ \mathscr{B}}(\xi) $. Sia $ C \in \K^{n} $ \[
            C=\begin{pmatrix}
                (w_1)_{ \mathscr{B}}\\
                \vdots\\
                (w_{h} )_{ \mathscr{B}}
            \end{pmatrix}
        \] Le righe di $ C $ sono le componenti dei votteri della base di $ W $ rispetto alla base $ B $, $ \null^{t}\!C=\big((w_1)_{ \mathscr{B}}, \cdots, (w_h)_{ \mathscr{B}}\big) $

        \[
            W^{\bot}=\big\{v \in V\,\tc\, \xi(w_{i}, v )=0\, \forall\, i=1, \cdots, h\big\}
        \]
        $ \xi(w_{i}, v)=0 $ $ \iff $ $ \null^{t}\!(w_{i} )_{ \mathscr{B}} A (v)_{ \mathscr{B}}=0 $ quindi \[
            W^{\bot}=\big\{v \in V\,\tc\, CA(v)_{ \mathscr{B}}=\underline{0}\big\}
        \]
        Quindi $ W^{\bot} $ sono i vettori $ v \in V $ tali che $ (v)_{ \mathscr{B}} \in \nulls(CA) $, $ \rank (C)=h $ e $ \rank (A)=n$\footnote{qui si usa $ \xi $ non degenere} , ovvero la dimensione di $ V $ 
        
        $\implies$ $ \rank(CA)=A $. Per il teorema di nullità più rango si ottiene che \[
            \dim \nulls (CA)=n-h= \dim V-\dim W\qedhere
        \]
    \end{itemize}
}

\subsection{Basi ortogonali, Teorema di Lagrange}

Sia $ V $ uno spazio vettoriale su $ \K $ (caratteristica di $ \K $ $ \neq 2 $), supponiamo $ V $ finitamente generato e $ \xi \in B_{S}(V, \K)  $. Sia $ \mathscr{B}=\{v_1, \cdots, v_{n}\}  $ una base di $ V $.

\definizione{}{
    $ \mathscr{B} $ è \textit{ortogonale} se $ \xi(v_{i}, v_j )=0 $ $ \forall\, i,j = 1, \cdots, n $, e $ i\neq j $.
}

\osservazione{
    Se $ B $ è una base ortogonale 
    
    $\implies$ $ M^{ \mathscr{B}}(\xi) $ è diagonale 
    
    $\implies$ $ Q_{\xi}(V)=\sum_{i=1}^{n} a_{ii} x_{i}^{2}$, dove $ (v)_{ \mathscr{B}}=(x_1, \cdots, x_{n} ) $
}

\teorema[(di Lagrange)]{diisoijsdoijsfodijflkjnhdskljjkjnkdsjklnsdkjnjnksdf}{
    Nelle nostre ipotesi esiste sempre una base ortogonale
}
\dimostrazione{diisoijsdoijsfodijflkjnhdskljjkjnkdsjklnsdkjnjnksdf}{
  Per induzione su $ n = \dim  V $.\begin{itemize}
      \item Se $ n=1 $, ogni base è ortogonale.
      \item Supponiamo l'enunciato vero per spazi vettoriali $ n $-dimensionali, e supponiamo $ \dim V=n+1 $
      \begin{itemize}
          \item Se $ \xi(v,w)=0 $ $ \forall\, v,w \in V $ 
      
          $\implies$ ogni base è ortogonale.
          \item Supponiamo che esista $ v_1 \in V $ tale che $ \xi(v_1,v_1)\neq 0 $
          
          Sia $ W= \mathscr{L}(v_1)^{\bot} $. \[
              W=\{v \in V\,\tc\, \xi(v, v_1)=0\}
          \] Sia \begin{align*}
          F_1:V & \to \K \\
          v & \mapsto \xi(v, v_1)
          \end{align*} $ F_1 $ è lineare poiché $ \xi $ è bilineare, e $ W=\ker F_1 $, $ \dim \K=1 $ 
          
          $\implies$ $ \dim W\ge n $, ma poiché $ F_1(v_1)\neq 0 $ 
          
          $\implies$ $ \dim W = n $, ma $ W $ non contiene $ v_1 $ 
          
          $\implies$ $ V=W \oplus L(v_1) $

          Si usa l'ipotesi induttiva 
          
          $\implies$ $ \exists\, \{w_2, \cdots, w_{n+1} \} $ base ortogonale di $ W $ 
          
          $\implies$ $ \{v_1, w_2, \cdots, w_{n+1}\}  $ base ortogonale di $ V $. \qed
      \end{itemize}
  \end{itemize}  
}

\corollario{doijsdofijooikjnkjjkjnadjnkdkjdaoijoadoijodsafoij}{
    Ogni matrice $ A \in \K^{n,n} $ simmetrico è congruente ad una matrice diagonale, cioè esiste $ P \in \gl(n, \K) $ tale che $ P^{-1} AP $ è diagonale.
}{}

\osservazione{
    Se $ \K $ è algebricamente chiuso (es. $ \K=\C $) 
    
    $\implies$ ogni base ortogonale può essere modificata in modo che sia ortogonale e $ \xi(v_{i}, v_{i} ) \in{0,1} $, infatti se $ \xi(v_i, v_i)= a_{i}  $ con $ a_{i} \neq 0 $, poiché $ \K $ algebricamente chiuso $ \exists\, b_{i} \in \K$ tale che $ b_{i}^{2}=a_{i}$, e quindi sostituendo a $ v_{i}$, $ v_{i}/b_{i}$ si ottiene \[
        \xi \bigl(v_{i}/b_{i}, v_{i}/b_{i}\bigr)=1
    \]
}

\proposizione{eternoriposo}{
    Siano $ A, B \in \K^{n,n} $ due matrici simmetriche, con $ \K $ campo algebricamente chiuso.

    $ A, B $ sono simili $ \iff $ $ \rank A = \rank B $
}
\dimostrazioneprop{eternoriposo}{
    \begin{itemize}
        \item [``$\implies$''] Ovvia e sempre vera.
        \item [``$\impliedby$''] Per il teorema di Lagrange entrambe sono simili ad una matrice diagonale. Poiché $ \K $ è algebricamente chiuso, per le matrici diagonali si può assumere che abbiano solo $ 0 $ e $ 1 $ sulla diagonale. 
        
        $\implies$ $ \exists P, Q \in \gl(n, \K) $ tale che $ \null^{t}\!P A P =D_1 $ e $ \null^{t}\!Q B Q = D_2$, $ D_1$ e $ D_2 $ matrici diagonali aventi solo $ 0 $ e $ 1  $ sulla diagonale.
        
        Posso supporre \[
            D_1=\left(\begin{array}{@{}c|c@{}}
                \id_{r} & 0 \\
                \hline
                0 & 0 
            \end{array}\right)
        \] dove $ \id_{r}  $ è l'identità $ r\times r $ e \[
            D_2=\left(\begin{array}{@{}c|c@{}}
                \id_{s} & 0 \\
                \hline
                0 & 0 
            \end{array}\right)
        \] dove $ \id_{s}  $ è l'identità $ s\times s $

        Poiché $ \rank A=\rank B $ risulta $ r=s $ 
        
        $\implies$ $ D_1=D_2 $ 
        
        $\implies$ $ \null^{t}\!P A P = \null^{t}\!Q B Q $ 
        
        $\implies$ $ A, B $ simili, poiché stanno nella stessa classe di equivalenza.\qed
    \end{itemize}
}